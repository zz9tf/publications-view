from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time
import yaml
import re
from datetime import datetime

class GoogleScholarCrawler:
    """
    ğŸ” Google Scholar è®ºæ–‡çˆ¬è™«
    åŠŸèƒ½ï¼šä»Google Scholaré¡µé¢æŠ“å–è®ºæ–‡è¯¦ç»†ä¿¡æ¯å¹¶ä¿å­˜ä¸ºYAMLæ ¼å¼
    """
    
    def __init__(self, headless=False):
        """
        åˆå§‹åŒ–çˆ¬è™«é…ç½®
        
        Args:
            headless (bool): æ˜¯å¦ä½¿ç”¨æ— å¤´æµè§ˆå™¨æ¨¡å¼
        """
        self.options = Options()
        if headless:
            self.options.add_argument("--headless")
        self.options.add_argument("--disable-gpu")
        self.options.add_argument("--window-size=1200,800")
        self.options.add_argument("--no-sandbox")
        self.options.add_argument("--disable-dev-shm-usage")
        
        self.driver = webdriver.Chrome(options=self.options)
        self.wait = WebDriverWait(self.driver, 10)
        self.publications = []
        
    def get_all_papers(self, url):
        """
        ğŸ“š è·å–æ‰€æœ‰è®ºæ–‡åŸºæœ¬ä¿¡æ¯ï¼ˆæ ‡é¢˜å’Œé“¾æ¥ï¼‰
        
        Args:
            url (str): Google Scholarä¸ªäººä¸»é¡µURL
            
        Returns:
            list: åŒ…å«è®ºæ–‡æ ‡é¢˜å’Œé“¾æ¥çš„åˆ—è¡¨
        """
        print("ğŸš€ å¼€å§‹è®¿é—®Google Scholaré¡µé¢...")
        self.driver.get(url)
        
        # è‡ªåŠ¨ç‚¹å‡»"Show more"ç›´åˆ°æ²¡æœ‰æ›´å¤šå¯ç‚¹ ğŸ“‹
        print("ğŸ“‹ æ­£åœ¨åŠ è½½æ‰€æœ‰è®ºæ–‡...")
        while True:
            try:
                more_button = self.driver.find_element(By.ID, "gsc_bpf_more")
                if more_button.is_enabled():
                    more_button.click()
                    time.sleep(2)  # å¢åŠ ç­‰å¾…æ—¶é—´
                    print("  â³ åŠ è½½æ›´å¤šè®ºæ–‡...")
                else:
                    break
            except NoSuchElementException:
                print("  âœ… æ‰€æœ‰è®ºæ–‡å·²åŠ è½½å®Œæˆ")
                break
            except Exception as e:
                print(f"  âš ï¸ åŠ è½½è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
                break
        
        # è·å–æ‰€æœ‰è®ºæ–‡é“¾æ¥å’ŒåŸºç¡€ä¿¡æ¯ ğŸ”—
        paper_rows = self.driver.find_elements(By.CSS_SELECTOR, ".gsc_a_tr")
        papers = []
        
        for i, row in enumerate(paper_rows):
            try:
                # æå–æ ‡é¢˜å’Œé“¾æ¥
                title_elem = row.find_element(By.CSS_SELECTOR, ".gsc_a_t a")
                title = title_elem.text.strip()
                link = title_elem.get_attribute("href")
                
                if not title or not link:
                    continue
                
                paper_info = {"title": title, "link": link}
                
                # å°è¯•ä»ä¸»é¡µé¢æå–å¼•ç”¨æ¬¡æ•° ğŸ“ˆ
                try:
                    citation_elem = row.find_element(By.CSS_SELECTOR, ".gsc_a_c")
                    citation_text = citation_elem.text.strip()
                    if citation_text and citation_text.isdigit():
                        paper_info["citations_preview"] = int(citation_text)
                        print(f"    ğŸ“ˆ é¢„è§ˆå¼•ç”¨æ¬¡æ•° '{title[:30]}...': {citation_text}")
                except:
                    paper_info["citations_preview"] = 0
                
                # å°è¯•æå–å¹´ä»½ä¿¡æ¯ ğŸ“…
                try:
                    year_elem = row.find_element(By.CSS_SELECTOR, ".gsc_a_y")
                    year_text = year_elem.text.strip()
                    if year_text and year_text.isdigit():
                        paper_info["year_preview"] = int(year_text)
                        print(f"    ğŸ“… é¢„è§ˆå¹´ä»½ '{title[:30]}...': {year_text}")
                except:
                    paper_info["year_preview"] = None
                
                papers.append(paper_info)
                
            except Exception as e:
                print(f"âš ï¸ æå–ç¬¬{i+1}è¡Œè®ºæ–‡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    
    def extract_paper_details(self, paper_info):
        """
        ğŸ” ä»è®ºæ–‡è¯¦æƒ…é¡µé¢æå–è¯¦ç»†ä¿¡æ¯
        
        Args:
            paper_info (dict): åŒ…å«è®ºæ–‡åŸºæœ¬ä¿¡æ¯çš„å­—å…¸ï¼ŒåŒ…æ‹¬linkå’Œé¢„è§ˆä¿¡æ¯
            
        Returns:
            dict: åŒ…å«è®ºæ–‡è¯¦ç»†ä¿¡æ¯çš„å­—å…¸
        """
        try:
            paper_link = paper_info.get("link")
            print(f"  ğŸ” æ­£åœ¨æå–è®ºæ–‡è¯¦æƒ…...")
            self.driver.get(paper_link)
            time.sleep(2)
            
            details = {}
            
            # ä½¿ç”¨é¢„è§ˆä¿¡æ¯ä½œä¸ºåˆå§‹å€¼ ğŸ“‹
            if "citations_preview" in paper_info:
                details['citations'] = paper_info["citations_preview"]
                print(f"    ğŸ“ˆ ä½¿ç”¨é¢„è§ˆå¼•ç”¨æ¬¡æ•°: {details['citations']}")
            
            if "year_preview" in paper_info and paper_info["year_preview"]:
                details['year'] = paper_info["year_preview"]
                print(f"    ğŸ“… ä½¿ç”¨é¢„è§ˆå¹´ä»½: {details['year']}")
            
            # æå–æ ‡é¢˜ ğŸ“
            try:
                title_elem = self.wait.until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "#gsc_oci_title"))
                )
                details['title'] = title_elem.text.strip()
            except TimeoutException:
                details['title'] = "Unknown Title"
            
            # æå–æ‰€æœ‰ä½œè€… ğŸ‘¥
            try:
                authors_elem = self.driver.find_element(By.CSS_SELECTOR, ".gsc_oci_value[data-testid='authors']")
                authors_text = authors_elem.text.strip()
                # åˆ†å‰²ä½œè€…åå­—ï¼Œå¤„ç†å„ç§åˆ†éš”ç¬¦
                authors = [author.strip() for author in re.split(r',|;|and', authors_text) if author.strip()]
                details['authors'] = authors
            except NoSuchElementException:
                details['authors'] = ["Unknown Author"]
            
            # æå–æ‰€æœ‰å­—æ®µä¿¡æ¯ ğŸ“‹
            field_rows = self.driver.find_elements(By.CSS_SELECTOR, ".gsc_oci_field")
            
            for field in field_rows:
                try:
                    field_name = field.text.strip().lower()
                    value_elem = field.find_element(By.XPATH, "./following-sibling::div[1]")
                    value = value_elem.text.strip()
                    
                    # æ ¹æ®å­—æ®µç±»å‹å¤„ç†æ•°æ® ğŸ·ï¸
                    if "publication date" in field_name or "date" in field_name:
                        details['date'] = value
                        # æå–å¹´ä»½
                        year_match = re.search(r'\d{4}', value)
                        if year_match:
                            details['year'] = int(year_match.group())
                    
                    elif "journal" in field_name or "conference" in field_name or "venue" in field_name:
                        details['venue'] = value
                        # åˆ¤æ–­venueç±»å‹
                        if any(conf_word in value.lower() for conf_word in ['conference', 'workshop', 'symposium', 'proceedings']):
                            details['venueType'] = 'conference'
                        elif any(journal_word in value.lower() for journal_word in ['journal', 'transactions', 'letters']):
                            details['venueType'] = 'journal'
                        elif 'arxiv' in value.lower():
                            details['venueType'] = 'arxiv'
                        else:
                            details['venueType'] = 'other'
                    
                    elif "description" in field_name or "abstract" in field_name:
                        details['description'] = value
                    
                    elif "total citations" in field_name or "cited by" in field_name or "å¼•ç”¨" in field_name:
                        # æå–æ•°å­—å¼•ç”¨æ¬¡æ•°
                        citation_num = re.search(r'\d+', value)
                        if citation_num:
                            details['citations'] = int(citation_num.group())
                        else:
                            details['citations'] = value
                        print(f"    ğŸ“ˆ å¼•ç”¨æ¬¡æ•°: {details['citations']}")
                        
                except Exception as e:
                    print(f"    âš ï¸ æå–å­—æ®µæ—¶å‡ºé”™ '{field_name}': {e}")
                    continue
            
            # ç”Ÿæˆç®€çŸ­venueåç§° ğŸ·ï¸
            if 'venue' in details:
                details['venueShort'] = self.generate_venue_short(details['venue'])
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°citationï¼Œå°è¯•å…¶ä»–æ–¹æ³•æå– ğŸ“ˆ
            if 'citations' not in details:
                try:
                    # æ–¹æ³•1: æŸ¥æ‰¾"Cited by"é“¾æ¥
                    cited_by_elem = self.driver.find_element(By.CSS_SELECTOR, "a[href*='cites']")
                    if cited_by_elem:
                        cited_text = cited_by_elem.text.strip()
                        citation_match = re.search(r'(\d+)', cited_text)
                        if citation_match:
                            details['citations'] = int(citation_match.group(1))
                            print(f"    ğŸ“ˆ é€šè¿‡cited byé“¾æ¥æå–å¼•ç”¨æ¬¡æ•°: {details['citations']}")
                except:
                    pass
                
                # æ–¹æ³•2: æŸ¥æ‰¾citationè®¡æ•°å™¨
                if 'citations' not in details:
                    try:
                        citation_elements = self.driver.find_elements(By.CSS_SELECTOR, ".gsc_oci_value")
                        for elem in citation_elements:
                            text = elem.text.strip()
                            if text.isdigit():
                                details['citations'] = int(text)
                                print(f"    ğŸ“ˆ é€šè¿‡æ•°å­—å…ƒç´ æå–å¼•ç”¨æ¬¡æ•°: {details['citations']}")
                                break
                    except:
                        pass
                
                # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°ï¼Œè®¾ç½®ä¸º0
                if 'citations' not in details:
                    details['citations'] = 0
                    print(f"    ğŸ“ˆ æœªæ‰¾åˆ°å¼•ç”¨ä¿¡æ¯ï¼Œè®¾ç½®ä¸º: 0")
            
            # è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾ ğŸ·ï¸
            details['tags'] = self.generate_tags(details)
            
            return details
            
        except Exception as e:
            print(f"    âŒ æå–è®ºæ–‡è¯¦æƒ…æ—¶å‡ºé”™: {e}")
            return None
    
    def generate_venue_short(self, venue_full):
        """
        ğŸ·ï¸ ç”Ÿæˆvenueçš„ç®€çŸ­åç§°
        
        ä¼˜å…ˆçº§ï¼š
        1. æå–æ‹¬å·ä¸­çš„ç¼©å†™ (å¦‚ ISBI, CVPR)
        2. ä½¿ç”¨é¢„å®šä¹‰æ˜ å°„
        3. å¦‚æœåç§°è¾ƒçŸ­ï¼Œç›´æ¥ä½¿ç”¨åŸåç§°
        4. å¦åˆ™ç”Ÿæˆé¦–å­—æ¯ç¼©å†™
        
        Args:
            venue_full (str): å®Œæ•´çš„venueåç§°
            
        Returns:
            str: ç®€çŸ­çš„venueåç§°
        """
        if not venue_full:
            return "Unknown"
        
        # 1. ğŸ” é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰æ‹¬å·ä¸­çš„ç¼©å†™
        import re
        bracket_match = re.search(r'\(([A-Z]+[A-Z0-9]*)\)', venue_full)
        if bracket_match:
            acronym = bracket_match.group(1)
            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆç¼©å†™ï¼ˆä¸»è¦æ˜¯å¤§å†™å­—æ¯ï¼‰
            if len(acronym) >= 2 and acronym.isupper():
                print(f"    ğŸ·ï¸ æå–æ‹¬å·ç¼©å†™: {acronym}")
                return acronym
        
        # 2. ğŸ“š ä½¿ç”¨é¢„å®šä¹‰çš„å¸¸è§ç¼©å†™æ˜ å°„
        abbreviations = {
            'arxiv preprint': 'ArXiv',
            'nature machine intelligence': 'NMI',
            'ieee transactions on pattern analysis and machine intelligence': 'TPAMI',
            'journal of machine learning research': 'JMLR',
            'proceedings of the national academy of sciences': 'PNAS',
            'nature communications': 'Nat Commun',
            'nature methods': 'Nat Methods',
            'nature biotechnology': 'Nat Biotechnol',
            'science': 'Science',
            'cell': 'Cell'
        }
        
        venue_lower = venue_full.lower().strip()
        for full_name, short_name in abbreviations.items():
            if full_name in venue_lower:
                print(f"    ğŸ·ï¸ ä½¿ç”¨é¢„å®šä¹‰æ˜ å°„: {short_name}")
                return short_name
        
        # 3. ğŸ“ å¦‚æœåç§°è¾ƒçŸ­ï¼Œç›´æ¥ä½¿ç”¨åŸåç§°
        clean_venue = re.sub(r'\([^)]*\)', '', venue_full).strip()  # ç§»é™¤æ‹¬å·å†…å®¹
        words = clean_venue.split()
        
        if len(words) <= 2:
            print(f"    ğŸ·ï¸ åç§°è¾ƒçŸ­ï¼Œç›´æ¥ä½¿ç”¨: {clean_venue}")
            return clean_venue
        
        # 4. ğŸ”¤ ç”Ÿæˆé¦–å­—æ¯ç¼©å†™ï¼ˆæ’é™¤å¸¸è§ä»‹è¯å’Œè¿è¯ï¼‰
        stop_words = {'and', 'of', 'the', 'in', 'on', 'for', 'with', 'to', 'a', 'an'}
        meaningful_words = [word for word in words 
                          if len(word) > 1 and word.lower() not in stop_words and word[0].isalpha()]
        
        if len(meaningful_words) >= 2:
            acronym = ''.join([word[0].upper() for word in meaningful_words[:4]])  # æœ€å¤šå–4ä¸ªè¯
            print(f"    ğŸ·ï¸ ç”Ÿæˆé¦–å­—æ¯ç¼©å†™: {acronym}")
            return acronym
        else:
            # å¦‚æœå®åœ¨æå–ä¸å‡ºæ¥ï¼Œè¿”å›æ¸…ç†åçš„åŸåç§°
            print(f"    ğŸ·ï¸ æ— æ³•ç”Ÿæˆç¼©å†™ï¼Œä½¿ç”¨åŸåç§°: {clean_venue}")
            return clean_venue
    
    def generate_tags(self, details):
        """
        ğŸ·ï¸ åŸºäºè®ºæ–‡ä¿¡æ¯è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾
        
        Args:
            details (dict): è®ºæ–‡è¯¦ç»†ä¿¡æ¯
            
        Returns:
            list: ç”Ÿæˆçš„æ ‡ç­¾åˆ—è¡¨
        """
        tags = []
        title = details.get('title', '').lower()
        description = details.get('description', '').lower()
        venue = details.get('venue', '').lower()
        
        # æŠ€æœ¯ç›¸å…³æ ‡ç­¾
        tech_keywords = {
            'deep learning': 'Deep Learning',
            'machine learning': 'Machine Learning',
            'neural network': 'Neural Networks',
            'transformer': 'Transformer',
            'attention': 'Attention',
            'gnn': 'Graph Neural Networks',
            'graph neural': 'Graph Neural Networks',
            'federated learning': 'Federated Learning',
            'computer vision': 'Computer Vision',
            'nlp': 'NLP',
            'natural language': 'NLP'
        }
        
        # åº”ç”¨é¢†åŸŸæ ‡ç­¾
        domain_keywords = {
            'medical': 'Medical',
            'healthcare': 'Healthcare',
            'pathology': 'Pathology',
            'drug discovery': 'Drug Discovery',
            'molecular': 'Molecular',
            'quantum': 'Quantum Computing',
            'biomedical': 'Biomedical',
            'clinical': 'Clinical'
        }
        
        all_text = f"{title} {description} {venue}"
        
        # æ£€æŸ¥æŠ€æœ¯å…³é”®è¯
        for keyword, tag in tech_keywords.items():
            if keyword in all_text:
                tags.append(tag)
        
        # æ£€æŸ¥åº”ç”¨é¢†åŸŸå…³é”®è¯
        for keyword, tag in domain_keywords.items():
            if keyword in all_text:
                tags.append(tag)
        
        # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œæ·»åŠ ä¸€ä¸ªé€šç”¨æ ‡ç­¾
        if not tags:
            tags.append("Research")
        
        return list(set(tags))  # å»é‡
    
    def crawl_all_papers(self, base_url):
        """
        ğŸš€ çˆ¬å–æ‰€æœ‰è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            base_url (str): Google Scholarä¸ªäººä¸»é¡µURL
        """
        try:
            # è·å–æ‰€æœ‰è®ºæ–‡çš„åŸºæœ¬ä¿¡æ¯
            papers = self.get_all_papers(base_url)
            
            print(f"\nğŸ“‹ å¼€å§‹çˆ¬å– {len(papers)} ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯...")
            
            for i, paper in enumerate(papers, 1):
                print(f"\nğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(papers)} ç¯‡è®ºæ–‡: {paper['title'][:50]}...")
                
                details = self.extract_paper_details(paper)
                
                if details:
                    # ç”Ÿæˆå”¯ä¸€ID
                    paper_id = f"pub-{i:03d}"
                    details['id'] = paper_id
                    
                    self.publications.append(details)
                    print(f"  âœ… æˆåŠŸæå–è®ºæ–‡ä¿¡æ¯")
                else:
                    print(f"  âŒ æå–è®ºæ–‡ä¿¡æ¯å¤±è´¥")
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
                time.sleep(1)
            
            print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼æ€»å…±æˆåŠŸå¤„ç† {len(self.publications)} ç¯‡è®ºæ–‡")
            
        except Exception as e:
            print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    def save_to_yaml(self, filename="scraped_publications.yaml"):
        """
        ğŸ’¾ å°†çˆ¬å–çš„æ•°æ®ä¿å­˜ä¸ºYAMLæ–‡ä»¶
        
        Args:
            filename (str): ä¿å­˜çš„æ–‡ä»¶å
        """
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                yaml.dump(self.publications, f, default_flow_style=False, 
                         allow_unicode=True, sort_keys=False, indent=2)
            print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° {filename}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    def close(self):
        """
        ğŸ”’ å…³é—­æµè§ˆå™¨
        """
        self.driver.quit()
        print("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")

def main():
    """
    ğŸš€ ä¸»å‡½æ•°ï¼šæ‰§è¡Œçˆ¬è™«ä»»åŠ¡
    """
    # Google Scholarä¸ªäººä¸»é¡µURL
    url = "https://scholar.google.com/citations?hl=en&user=X7KrguAAAAAJ&view_op=list_works&sortby=pubdate"
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = GoogleScholarCrawler(headless=False)  # è®¾ç½®ä¸ºTrueå¯å¯ç”¨æ— å¤´æ¨¡å¼
    
    try:
        # å¼€å§‹çˆ¬å–
        crawler.crawl_all_papers(url)
        
        # ä¿å­˜æ•°æ®
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"publications_{timestamp}.yaml"
        crawler.save_to_yaml(filename)
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
    finally:
        # ç¡®ä¿å…³é—­æµè§ˆå™¨
        crawler.close()

if __name__ == "__main__":
    main()
