"use client";

import React, { useState } from "react";
import Header from "@/components/Header";
import PaperSidebar from "@/components/PaperSidebar";
import PaperDetailPanel from "@/components/PaperDetailPanel";
import { Paper } from "@/types";

/**
 * ä¸»é¡µé¢ç»„ä»¶ - Paper Viewåº”ç”¨çš„ä¸»ç•Œé¢
 * @returns {JSX.Element} Homeç»„ä»¶
 */
export default function Home() {
  // çŠ¶æ€ç®¡ç† ğŸ¯
  const [leftSidebarCollapsed, setLeftSidebarCollapsed] = useState(false);
  const [rightPanelCollapsed, setRightPanelCollapsed] = useState(false);
  const [selectedPaper, setSelectedPaper] = useState<Paper | null>(null);

  // æ¨¡æ‹Ÿè®ºæ–‡æ•°æ® ğŸ“š
  const mockPapers: Paper[] = [
    {
      id: "1",
      title:
        "CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation",
      authors: [
        "Zhenyi Shen",
        "Heng Yan",
        "Linhai Zhang",
        "Zhanghao Hu",
        "Yali Du",
      ],
      year: 2025,
      citations: 4,
      type: "Origin paper",
      abstract:
        "Chain-of-Thought (CoT) enhances Large Language Models (LLMs) by enabling step-by-step reasoning in natural language. However, the language space may be suboptimal for reasoning. While implicit CoT methods attempt to enable reasoning without explicit CoT tokens, they have consistently lagged behind explicit CoT method in task performance. We propose CODI (Continuous Chain-of-Thought via Self-Distillation), a novel framework that distills CoT into a continuous space, where a shared model acts as both teacher and student, jointly learning explicit and implicit CoT while aligning their hidden activation on the token generating the final answer. CODI is the first implicit CoT method to match explicit CoT's performance on GSM8k while achieving 3.1x compression, surpassing the previous state-of-the-art by 28.2% in accuracy. Furthermore, CODI demonstrates scalability, robustness, and generalizability to more complex CoT datasets. Additionally, CODI retains interpretability by decoding its continuous thoughts, making its reasoning process transparent. Our findings establish implicit CoT as not only a more efficient but a powerful alternative to explicit CoT.",
      arxivId: "arXiv.org",
      publisher: "arXiv.org",
      keywords: [
        "Chain-of-Thought",
        "Large Language Models",
        "Self-Distillation",
        "Reasoning",
      ],
    },
    {
      id: "2",
      title:
        "Distilling Reasoning Ability from Large Language Models with Adaptive Thinking",
      authors: ["Xiao Chen", "Sihang Zhou", "K. Liang", "Xinwang Liu"],
      year: 2024,
      citations: 12,
      type: "Original paper",
      abstract:
        "This paper presents a novel approach to distill reasoning capabilities from large language models through adaptive thinking mechanisms.",
      keywords: ["Reasoning", "Distillation", "Adaptive Thinking"],
    },
    {
      id: "3",
      title: "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs",
      authors: ["Yige Xu", "Xu Guo", "Zhiwei Zeng", "Chunyan Miao"],
      year: 2025,
      citations: 8,
      type: "Original paper",
      abstract:
        "We introduce SoftCoT, a method for efficient reasoning in large language models using soft chain-of-thought approaches.",
      keywords: ["Chain-of-Thought", "Efficiency", "LLMs"],
    },
    {
      id: "4",
      title: "TokenSkip: Controllable Chain-of-Thought Compression in LLMs",
      authors: [
        "Henming Xia",
        "Yongqi Li",
        "Chak Tou Leong",
        "Wenjie Wang",
        "Wenjie Li",
      ],
      year: 2025,
      citations: 5,
      type: "Conference paper",
      abstract:
        "TokenSkip presents a controllable approach to compress chain-of-thought reasoning in large language models.",
      keywords: ["Token Compression", "Chain-of-Thought", "Control"],
    },
    {
      id: "5",
      title:
        "Democratizing Reasoning Ability: Tailored Learning from Large Language Model",
      authors: ["Zhaoyang Wang", "Shaohan Huang", "Yuzuan Liu", "Jiahai Wang"],
      year: 2023,
      citations: 15,
      type: "Journal paper",
      abstract:
        "This work focuses on democratizing reasoning abilities through tailored learning approaches.",
      keywords: ["Democratization", "Reasoning", "Tailored Learning"],
    },
  ];

  /**
   * å¤„ç†è®ºæ–‡é€‰æ‹© ğŸ“–
   * @param {Paper} paper - é€‰ä¸­çš„è®ºæ–‡
   */
  const handleSelectPaper = (paper: Paper) => {
    setSelectedPaper(paper);
    // å¦‚æœå³ä¾§é¢æ¿è¢«æ”¶ç¼©äº†ï¼Œè‡ªåŠ¨å±•å¼€
    if (rightPanelCollapsed) {
      setRightPanelCollapsed(false);
    }
  };

  /**
   * åˆ‡æ¢å·¦ä¾§è¾¹æ çŠ¶æ€ â¬…ï¸
   */
  const toggleLeftSidebar = () => {
    setLeftSidebarCollapsed(!leftSidebarCollapsed);
  };

  /**
   * åˆ‡æ¢å³ä¾§é¢æ¿çŠ¶æ€ â¡ï¸
   */
  const toggleRightPanel = () => {
    setRightPanelCollapsed(!rightPanelCollapsed);
  };

  return (
    <div className="h-screen flex flex-col bg-gray-50">
      {/* é¡¶éƒ¨å¯¼èˆªæ  */}
      <Header />

      {/* ä¸»å†…å®¹åŒºåŸŸ */}
      <div className="flex-1 flex overflow-hidden">
        {/* å·¦ä¾§è¾¹æ  - è®ºæ–‡åˆ—è¡¨ */}
        <PaperSidebar
          isCollapsed={leftSidebarCollapsed}
          onToggle={toggleLeftSidebar}
          papers={mockPapers}
          selectedPaper={selectedPaper}
          onSelectPaper={handleSelectPaper}
        />

        {/* ä¸­é—´å†…å®¹åŒºåŸŸ - ä¿æŒç©ºç™½ */}
        <div className="flex-1 bg-white">
          {/* è¿™é‡Œç•™ç©ºï¼Œç”¨äºåç»­åŠŸèƒ½å¼€å‘ */}
        </div>

        {/* å³ä¾§è¯¦æƒ…é¢æ¿ */}
        <PaperDetailPanel
          isCollapsed={rightPanelCollapsed}
          onToggle={toggleRightPanel}
          selectedPaper={selectedPaper}
        />
      </div>
    </div>
  );
}
